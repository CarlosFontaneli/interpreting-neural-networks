{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../models/\")\n",
    "\n",
    "# Add the upper directory to the path\n",
    "sys.path.append(\"../models/\")\n",
    "from CustomCNNVessel import CustomResNet\n",
    "sys.path.append(\"../data/\")\n",
    "from VessMapDatasetLoader import vess_map_dataloader\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloaders\n",
    "image_dir = '/home/fonta42/Desktop/interpretacao-redes-neurais/data/VessMap/images'\n",
    "mask_dir = '/home/fonta42/Desktop/interpretacao-redes-neurais/data/VessMap/labels'\n",
    "skeleton_dir = '/home/fonta42/Desktop/interpretacao-redes-neurais/data/VessMap/skeletons'\n",
    "\n",
    "batch_size = 10\n",
    "train_size = 0.8\n",
    "\n",
    "train_loader, test_loader = vess_map_dataloader(image_dir, \n",
    "                                  mask_dir, \n",
    "                                  skeleton_dir, \n",
    "                                  batch_size,\n",
    "                                  train_size = train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "model = CustomResNet(num_classes=2).to(device)\n",
    "# Load the weights\n",
    "model.load_state_dict(torch.load(f\"../models/vess_map_regularized_none_200.pth\"))\n",
    "\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images = []\n",
    "\n",
    "\n",
    "# Iterate through the entire train_loader\n",
    "for batch in train_loader:\n",
    "    images, _, _ = batch\n",
    "    images = images.to(device)\n",
    "    all_images.extend(images)\n",
    "    \n",
    "for batch in test_loader:\n",
    "    images, _, _ = batch\n",
    "    images = images.to(device)\n",
    "    all_images.extend(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_softmax_gradient(model, image, x_pixel, y_pixel, threshold_proportion):\n",
    "    # Ensure image requires gradient\n",
    "    image = image.unsqueeze(0).requires_grad_()\n",
    "\n",
    "    # Forward pass\n",
    "    out = model(image)\n",
    "\n",
    "    # Apply softmax to the output to get class probabilities\n",
    "    probabilities = F.softmax(out, dim=1)\n",
    "\n",
    "    score = probabilities[0, 1, x_pixel, y_pixel]  # Probability of class 1 at (x, y)\n",
    "\n",
    "    # Compute gradients\n",
    "    score.backward()\n",
    "\n",
    "    gradient = image.grad.squeeze().cpu().numpy()\n",
    "    # Return the gradient\n",
    "    threshold = np.max(gradient) * threshold_proportion\n",
    "    mask = np.abs(gradient) > threshold\n",
    "    non_zero_coords = np.nonzero(mask)\n",
    "    \n",
    "    if len(non_zero_coords[0]) > 0:\n",
    "        y_min, y_max = non_zero_coords[0].min(), non_zero_coords[0].max()\n",
    "        x_min, x_max = non_zero_coords[1].min(), non_zero_coords[1].max()\n",
    "        num_pixels_above_threshold = np.sum(mask)\n",
    "        bounding_box_area = (y_max - y_min + 1) * (x_max - x_min + 1)\n",
    "        #fulfillment = num_pixels_above_threshold / bounding_box_area\n",
    "        return bounding_box_area\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_gradients(model, image, sampling_rate = 10, device = \"cuda\", vectorize = False):\n",
    "  torch.cuda.empty_cache()\n",
    "\n",
    "  model.to(device)\n",
    "  image = image.to(device).unsqueeze(0)\n",
    "  sampled_image = image[:,:,::sampling_rate,::sampling_rate]\n",
    "  sampled_image.requires_grad = True\n",
    "  \n",
    "  jacobian_gradient = torch.autograd.functional.jacobian(model, \n",
    "                                                         sampled_image,\n",
    "                                                         vectorize = vectorize)\n",
    "  jacobian_gradient = jacobian_gradient.squeeze()\n",
    "  return jacobian_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bounding_box_area(model, image, threshold_proportion):\n",
    "    \n",
    "    # Dimensions of the original image\n",
    "    height, width = image.shape[1], image.shape[2]\n",
    "    # Sampling ratio\n",
    "    sampling = 5\n",
    "\n",
    "    jacobian_gradients = get_all_gradients(model, image, sampling_rate = sampling, device = 'cuda')\n",
    "    \n",
    "    # Generate arrays of indices for rows and columns\n",
    "    row_indices = np.arange(0, height, sampling)\n",
    "    col_indices = np.arange(0, width, sampling)\n",
    "\n",
    "    # Create a meshgrid of indices\n",
    "    row_indices, col_indices = np.meshgrid(row_indices, col_indices)\n",
    "\n",
    "    # Flatten the arrays to get a list of (row, col) pairs\n",
    "    sampled_pixel_indices = np.vstack([row_indices.flatten(), col_indices.flatten()]).T\n",
    "\n",
    "    fulfillment_image = np.zeros_like(image.to('cpu'))\n",
    "    for pixel_coord in sampled_pixel_indices:\n",
    "        # Return the gradient\n",
    "        \n",
    "        gradient = jacobian_gradients[1, int(pixel_coord[0] / 10), int(pixel_coord[1] / 10)]\n",
    "        \n",
    "        threshold = np.max(gradient.detach().cpu().numpy()) * threshold_proportion\n",
    "        mask = np.abs(gradient.detach().cpu().numpy()) > threshold\n",
    "        non_zero_coords = np.nonzero(mask)\n",
    "        \n",
    "        if len(non_zero_coords[0]) > 0:\n",
    "            y_min, y_max = non_zero_coords[0].min(), non_zero_coords[0].max()\n",
    "            x_min, x_max = non_zero_coords[1].min(), non_zero_coords[1].max()\n",
    "            num_pixels_above_threshold = np.sum(mask)\n",
    "            bounding_box_area = (y_max - y_min + 1) * (x_max - x_min + 1)\n",
    "            #fulfillment = num_pixels_above_threshold / bounding_box_area\n",
    "            fulfillment_image[0, pixel_coord[0], pixel_coord[1]] = bounding_box_area\n",
    "        \n",
    "    return fulfillment_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 images processed\n",
      "10 images processed\n",
      "20 images processed\n",
      "30 images processed\n",
      "40 images processed\n",
      "50 images processed\n",
      "60 images processed\n",
      "70 images processed\n",
      "80 images processed\n",
      "90 images processed\n"
     ]
    }
   ],
   "source": [
    "threshold_proportion = 0.1\n",
    "fulfillment_image = np.zeros_like(all_images[0].to('cpu'))  # Initialize the fulfillment image\n",
    "fulfillment_images = []\n",
    "for idx, input_image in enumerate(all_images):\n",
    "    fulfillment_image = compute_bounding_box_area(model, input_image, threshold_proportion)\n",
    "\n",
    "    # Normalize the fulfillment image to the range [0, 1]\n",
    "    fulfillment_image = (fulfillment_image - np.min(fulfillment_image)) / (np.max(fulfillment_image) - np.min(fulfillment_image))\n",
    "\n",
    "    fulfillment_images.append(fulfillment_image)\n",
    "    if idx % 10 == 0:\n",
    "        print(f'{idx} images processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "# Function to save images in a directory\n",
    "def save_images(image_array, directory_name):\n",
    "    # Create the directory if it does not exist\n",
    "    if not os.path.exists(directory_name):\n",
    "        os.makedirs(directory_name)\n",
    "\n",
    "    # Loop through the images and save them\n",
    "    for i, img_tensor in enumerate(image_array):\n",
    "        # Convert the tensor to a PIL image\n",
    "        img = TF.to_pil_image(img_tensor)\n",
    "        #print(img_tensor.squeeze(0).transpose(1, 2, 0).shape)\n",
    "        # Convert the image to 8-bit grayscale if it's in mode 'F'\n",
    "        if img.mode == 'F':\n",
    "            img = img.convert('L')\n",
    "\n",
    "        # Save the image with reduced file size\n",
    "        img.save(os.path.join(directory_name, f'image_{i}.png'), 'PNG', optimize=True, quality=20)\n",
    "\n",
    "# Save all_images and fulfillment_images\n",
    "save_images(all_images, './original_images')\n",
    "#save_images(fulfillment_images, './fulfillment_images')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 224, 224)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fulfillment_images = [np.expand_dims(img, axis=0) for img in fulfillment_images]\n",
    "fulfillment_images[25].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_and_save_images(image_array, directory_name):\n",
    "    # Create the directory if it does not exist\n",
    "    if not os.path.exists(directory_name):\n",
    "        os.makedirs(directory_name)\n",
    "\n",
    "    # Loop through the images and plot/save them\n",
    "    for i, img in enumerate(image_array):\n",
    "        plt.figure(figsize=(224/6, 224/6), dpi=6)\n",
    "        plt.imshow(img.transpose(1, 2, 0), cmap='RdYlGn')\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Save the plot as a PNG file\n",
    "        plt.savefig(os.path.join(directory_name, f'image_{i}.png'), bbox_inches='tight', pad_inches=0)\n",
    "        plt.close()  # Close the figure to free up memory\n",
    "\n",
    "# Example usage\n",
    "plot_and_save_images(fulfillment_images[:1], './fulfillment_images')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(220, 221, 4)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = Image.open('/home/fonta42/Desktop/interpretacao-redes-neurais/experiments/fulfillment_images/image_0.png')\n",
    "np.array(img).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
