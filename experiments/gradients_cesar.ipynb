{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'CustomCNNVessel'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4c2fd26d8dc5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjacobian\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mCustomCNNVessel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCustomResNet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_all_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampling_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'CustomCNNVessel'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd.functional import jacobian\n",
    "sys.path.append(\"../../models/\")\n",
    "from CustomCNNVessel import CustomResNet\n",
    "\n",
    "def get_all_gradients(model, image, sampling_rate=10, device=\"cuda\", vectorize=False):\n",
    "  torch.cuda.empty_cache()\n",
    "\n",
    "  model.to(device)\n",
    "  model = model.eval()\n",
    "  image = image.to(device).unsqueeze(0)\n",
    "  sampled_image = image[:,:,::sampling_rate,::sampling_rate].to(device)\n",
    "  sampled_image.requires_grad = True\n",
    "  \n",
    "  jacobian_gradient = torch.autograd.functional.jacobian(model, \n",
    "                                                         sampled_image,\n",
    "                                                         vectorize = vectorize)\n",
    "  jacobian_gradient = jacobian_gradient.squeeze().to('cpu')\n",
    "  return jacobian_gradient\n",
    "\n",
    "def wrapper(model, first_row, last_row):\n",
    "    '''Wrap model to return only rows in range [first_row, last_row]'''\n",
    "    def new_model(img):\n",
    "        out = model(img)\n",
    "        return out[:,:,first_row:last_row]\n",
    "    return new_model\n",
    "\n",
    "def get_all_gradients_chunks(model, image, rows_proc=28, device=\"cuda\"):\n",
    "\n",
    "  c, nr, nc = image.shape\n",
    "  model.eval()\n",
    "  model.to(device)\n",
    "\n",
    "  size_jac = (2, nr, nc, nr, nc)\n",
    "  # Number of image chunks to process\n",
    "  nchunks = nr//rows_proc\n",
    "  nchunks += nr%rows_proc!=0   # Add 1 if shape is not divisible by rows_proc\n",
    "  image_cuda = image.unsqueeze(0).to(device).requires_grad_()\n",
    "\n",
    "  jacobian_gradient = torch.zeros(*size_jac, device='cpu')\n",
    "  first_row = 0\n",
    "  for idx in range(nchunks):\n",
    "      print(idx)\n",
    "      last_row = first_row + rows_proc\n",
    "      model_w = wrapper(model, first_row, last_row)\n",
    "      out = jacobian(model_w, image_cuda, vectorize=False).to('cpu')  \n",
    "      # Cannot use .squeeze here since one valid size of out might be 1\n",
    "      jacobian_gradient[:,first_row:last_row] = out[0,:,:,:,0,0]\n",
    "      first_row = last_row\n",
    "\n",
    "  return jacobian_gradient\n",
    "\n",
    "def compare(model, image, rows_proc):\n",
    "  '''Compare methods'''\n",
    "   \n",
    "  torch.backends.cudnn.deterministic = True\n",
    "  ref = get_all_gradients(model, image, sampling_rate=1)\n",
    "  jacobian_gradient = get_all_gradients_chunks(model, image, rows_proc)\n",
    "  print(torch.allclose(ref, jacobian_gradient))\n",
    "\n",
    "model = CustomResNet(num_classes=2)\n",
    "rows_proc = 4   # Process image rows_proc at a time\n",
    "image = torch.rand(1, 224, 224)[:,:10,:10]\n",
    "\n",
    "jacobian_gradient = get_all_gradients_chunks(model, image, rows_proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradients_in_batches(model, image, batch_size, device=\"cpu\"):\n",
    "    c, h, w = image.shape\n",
    "    # Initialize a tensor to hold the full gradient\n",
    "    full_gradient = torch.zeros((c, h, w), device='cpu')\n",
    "\n",
    "    for i in range(0, h, batch_size):\n",
    "        for j in range(0, w, batch_size):\n",
    "            # Calculate the size of the current batch\n",
    "            current_batch_size_h = min(batch_size, h - i)\n",
    "            current_batch_size_w = min(batch_size, w - j)\n",
    "\n",
    "            # Extract a batch from the image\n",
    "            batch = image[:, i:i + current_batch_size_h, j:j + current_batch_size_w].unsqueeze(0).to(device)\n",
    "            batch.requires_grad = True\n",
    "\n",
    "            # Compute the gradient for the batch\n",
    "            output = model(batch)\n",
    "            output.backward(torch.ones_like(output))\n",
    "            batch_gradient = batch.grad.squeeze().cpu()\n",
    "\n",
    "            # Assign the batch gradient to the corresponding region in the full gradient tensor\n",
    "            full_gradient[:, i:i + current_batch_size_h, j:j + current_batch_size_w] = batch_gradient\n",
    "\n",
    "            # Free memory\n",
    "            del batch, output, batch_gradient\n",
    "            if device == \"cuda\":\n",
    "                torch.cuda.empty_cache()  # Release GPU memory\n",
    "\n",
    "    return full_gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "compare(model, image, rows_proc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
